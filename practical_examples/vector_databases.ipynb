{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c104c1d5",
   "metadata": {},
   "source": [
    "# Vector Databases: Practical Examples for Software Teams\n",
    "\n",
    "## Overview\n",
    "This notebook provides hands-on examples of vector databases and embeddings concepts to complement the AI presentation. We'll explore:\n",
    "\n",
    "- üß† **Understanding Embeddings**: How text becomes numerical vectors\n",
    "- üîç **Semantic Search**: Finding meaning, not just keywords  \n",
    "- üìö **RAG Implementation**: Retrieval Augmented Generation in practice\n",
    "- ‚ö° **Performance Comparisons**: Traditional vs Vector search\n",
    "- üöÄ **Production Tools**: ChromaDB and scaling considerations\n",
    "\n",
    "---\n",
    "\n",
    "## Why Vector Databases Matter for Software Teams\n",
    "\n",
    "**Traditional Problems:**\n",
    "- Keyword search misses semantic meaning\n",
    "- Documentation scattered across tools\n",
    "- Code search limited to exact matches\n",
    "- Knowledge silos in large codebases\n",
    "\n",
    "**Vector Database Solutions:**\n",
    "- Find documents by meaning, not just words\n",
    "- Semantic code search across repositories  \n",
    "- Intelligent documentation retrieval\n",
    "- Context-aware AI assistance (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063ff3a9",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "First, let's install and import the required libraries. Run these cells to set up your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install sentence-transformers chromadb numpy pandas matplotlib seaborn scikit-learn\n",
    "\n",
    "# If running in Colab, uncomment the line above\n",
    "# For local environments, install via: pip install sentence-transformers chromadb numpy pandas matplotlib seaborn scikit-learn\n",
    "\n",
    "print(\"üì¶ Installing packages... (uncomment the pip install line above if needed)\")\n",
    "print(\"‚úÖ Ready to proceed with imports!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f0f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Try importing sentence-transformers and chromadb\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    print(\"‚úÖ sentence-transformers imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå sentence-transformers not found. Please install: pip install sentence-transformers\")\n",
    "\n",
    "try:\n",
    "    import chromadb\n",
    "    print(\"‚úÖ chromadb imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå chromadb not found. Please install: pip install chromadb\")\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"\\nüöÄ Setup complete! Ready to explore vector databases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab46ab4a",
   "metadata": {},
   "source": [
    "## 2. Understanding Embeddings\n",
    "\n",
    "**What are embeddings?** \n",
    "Numerical representations of text that capture semantic meaning. Similar texts produce similar vectors.\n",
    "\n",
    "**Analogy:** Like converting books into unique barcodes based on their content, not just title.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c0ae0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained sentence transformer model\n",
    "# This model converts text to 384-dimensional vectors\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Example texts with different semantic meanings\n",
    "texts = [\n",
    "    \"Python is a programming language\",\n",
    "    \"Python is a type of snake\", \n",
    "    \"JavaScript is used for web development\",\n",
    "    \"Java is an object-oriented programming language\",\n",
    "    \"Snakes are reptiles that slither\",\n",
    "    \"Web development involves HTML, CSS, and JavaScript\",\n",
    "    \"Machine learning algorithms process data\",\n",
    "    \"Data science uses statistics and programming\"\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "embeddings = model.encode(texts)\n",
    "\n",
    "print(f\"üìä Generated embeddings:\")\n",
    "print(f\"   ‚Ä¢ Number of texts: {len(texts)}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimensions: {embeddings.shape[1]}\")\n",
    "print(f\"   ‚Ä¢ Shape: {embeddings.shape}\")\n",
    "print(f\"\\nüîç Example embedding (first 10 dimensions):\")\n",
    "print(f\"   '{texts[0]}' -> {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc1e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Create a heatmap to visualize similarities\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='viridis',\n",
    "            xticklabels=[f\"{i}: {text[:30]}...\" for i, text in enumerate(texts)],\n",
    "            yticklabels=[f\"{i}: {text[:30]}...\" for i, text in enumerate(texts)])\n",
    "\n",
    "plt.title('üìä Embedding Similarity Matrix\\n(Darker = More Similar)', fontsize=14, pad=20)\n",
    "plt.xlabel('Texts', fontsize=12)\n",
    "plt.ylabel('Texts', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üîç Key Observations:\")\n",
    "print(f\"   ‚Ä¢ Programming texts cluster together (higher similarity)\")\n",
    "print(f\"   ‚Ä¢ 'Python language' vs 'Python snake': {similarity_matrix[0][1]:.3f}\")\n",
    "print(f\"   ‚Ä¢ 'Python language' vs 'Java language': {similarity_matrix[0][3]:.3f}\")\n",
    "print(f\"   ‚Ä¢ 'Python snake' vs 'Snakes are reptiles': {similarity_matrix[1][4]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace7532c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings in 2D using PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], \n",
    "                     c=range(len(texts)), cmap='tab10', s=100, alpha=0.7)\n",
    "\n",
    "# Add labels for each point\n",
    "for i, txt in enumerate(texts):\n",
    "    plt.annotate(f\"{i}: {txt[:25]}...\", \n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=9, alpha=0.8)\n",
    "\n",
    "plt.title('üó∫Ô∏è Embedding Visualization (2D Projection via PCA)', fontsize=14, pad=20)\n",
    "plt.xlabel(f'PCA Component 1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PCA Component 2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üéØ Notice how semantically similar texts cluster together:\")\n",
    "print(\"   ‚Ä¢ Programming languages are grouped\")\n",
    "print(\"   ‚Ä¢ Animals/snake references are grouped\")\n",
    "print(\"   ‚Ä¢ Web development concepts are grouped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa4bb18",
   "metadata": {},
   "source": [
    "## 3. Creating a Simple Vector Database\n",
    "\n",
    "Now let's build a basic vector database from scratch using NumPy. This will help understand what happens under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce409b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorDB:\n",
    "    \"\"\"A basic vector database implementation using NumPy\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.documents = []\n",
    "        self.embeddings = None\n",
    "        \n",
    "    def add_documents(self, docs):\n",
    "        \"\"\"Add documents to the database\"\"\"\n",
    "        self.documents.extend(docs)\n",
    "        # Generate embeddings for all documents\n",
    "        all_embeddings = self.model.encode(self.documents)\n",
    "        self.embeddings = np.array(all_embeddings)\n",
    "        print(f\"‚úÖ Added {len(docs)} documents. Total: {len(self.documents)}\")\n",
    "        \n",
    "    def search(self, query, top_k=3):\n",
    "        \"\"\"Search for similar documents\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "            \n",
    "        # Generate embedding for query\n",
    "        query_embedding = self.model.encode([query])\n",
    "        \n",
    "        # Calculate similarities\n",
    "        similarities = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # Get top-k most similar\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'document': self.documents[idx],\n",
    "                'similarity': similarities[idx],\n",
    "                'index': idx\n",
    "            })\n",
    "            \n",
    "        return results\n",
    "\n",
    "# Create our vector database\n",
    "vector_db = SimpleVectorDB(model)\n",
    "\n",
    "# Sample software engineering documents\n",
    "documents = [\n",
    "    \"Python Django REST API development tutorial\",\n",
    "    \"React components and state management patterns\", \n",
    "    \"Database schema design best practices\",\n",
    "    \"Git branching strategies for team collaboration\",\n",
    "    \"Docker containerization for microservices\",\n",
    "    \"JavaScript asynchronous programming with promises\",\n",
    "    \"SQL query optimization techniques\",\n",
    "    \"AWS cloud infrastructure deployment\",\n",
    "    \"Test-driven development with Jest and pytest\",\n",
    "    \"API authentication using JWT tokens\",\n",
    "    \"Redis caching for application performance\",\n",
    "    \"Kubernetes container orchestration guide\"\n",
    "]\n",
    "\n",
    "vector_db.add_documents(documents)\n",
    "print(f\"üìö Vector database ready with {len(documents)} documents!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c71279c",
   "metadata": {},
   "source": [
    "## 4. Semantic Search Implementation\n",
    "\n",
    "Now let's test our vector database with semantic search queries. Notice how it finds relevant documents even without exact keyword matches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c3e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_search(query, description=\"\"):\n",
    "    \"\"\"Demonstrate search results for a query\"\"\"\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    if description:\n",
    "        print(f\"   üìù {description}\")\n",
    "    \n",
    "    results = vector_db.search(query, top_k=3)\n",
    "    \n",
    "    for i, result in enumerate(results, 1):\n",
    "        print(f\"   {i}. [{result['similarity']:.3f}] {result['document']}\")\n",
    "\n",
    "# Test various semantic search scenarios\n",
    "demo_search(\n",
    "    \"building web applications\", \n",
    "    \"Should find React, Django, etc. even though query doesn't contain exact words\"\n",
    ")\n",
    "\n",
    "demo_search(\n",
    "    \"database performance optimization\",\n",
    "    \"Should find SQL optimization and caching docs\"\n",
    ")\n",
    "\n",
    "demo_search(\n",
    "    \"deployment and infrastructure\",\n",
    "    \"Should find Docker, AWS, Kubernetes content\"\n",
    ")\n",
    "\n",
    "demo_search(\n",
    "    \"team development workflow\",\n",
    "    \"Should find Git branching and testing practices\"\n",
    ")\n",
    "\n",
    "demo_search(\n",
    "    \"frontend component architecture\",\n",
    "    \"Should prioritize React components over backend topics\"\n",
    ")\n",
    "\n",
    "print(f\"\\nüí° Key Insight: Vector search finds semantically related content,\")\n",
    "print(f\"    not just exact keyword matches!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48df43d",
   "metadata": {},
   "source": [
    "## 5. Document Similarity Analysis\n",
    "\n",
    "Let's analyze how our documents cluster together and find the most similar pairs. This is useful for organizing documentation, finding duplicates, or understanding topic relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e65140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix for all documents\n",
    "doc_similarities = cosine_similarity(vector_db.embeddings)\n",
    "\n",
    "# Find most similar document pairs (excluding self-similarity)\n",
    "similar_pairs = []\n",
    "for i in range(len(documents)):\n",
    "    for j in range(i+1, len(documents)):\n",
    "        similarity = doc_similarities[i][j]\n",
    "        similar_pairs.append({\n",
    "            'doc1_idx': i,\n",
    "            'doc2_idx': j,\n",
    "            'doc1': documents[i][:50] + \"...\",\n",
    "            'doc2': documents[j][:50] + \"...\",\n",
    "            'similarity': similarity\n",
    "        })\n",
    "\n",
    "# Sort by similarity and show top pairs\n",
    "similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "print(\"üîó Most Similar Document Pairs:\")\n",
    "print(\"=\" * 80)\n",
    "for i, pair in enumerate(similar_pairs[:5], 1):\n",
    "    print(f\"{i}. Similarity: {pair['similarity']:.3f}\")\n",
    "    print(f\"   üìÑ Doc {pair['doc1_idx']}: {pair['doc1']}\")\n",
    "    print(f\"   üìÑ Doc {pair['doc2_idx']}: {pair['doc2']}\")\n",
    "    print()\n",
    "\n",
    "# Visualize document similarity matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(doc_similarities, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='viridis',\n",
    "            xticklabels=[f\"{i}: {doc[:20]}...\" for i, doc in enumerate(documents)],\n",
    "            yticklabels=[f\"{i}: {doc[:20]}...\" for i, doc in enumerate(documents)])\n",
    "\n",
    "plt.title('üìä Document Similarity Matrix', fontsize=14, pad=20)\n",
    "plt.xlabel('Documents', fontsize=12)\n",
    "plt.ylabel('Documents', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1890ca",
   "metadata": {},
   "source": [
    "## 6. RAG (Retrieval Augmented Generation) Example\n",
    "\n",
    "RAG combines vector search with language model generation. Let's build a simple RAG system that retrieves relevant context and generates responses.\n",
    "\n",
    "**Note:** This example demonstrates the retrieval part. In production, you'd combine this with an LLM API (OpenAI, Anthropic, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRAG:\n",
    "    \"\"\"A basic RAG system for demonstration\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_db):\n",
    "        self.vector_db = vector_db\n",
    "        \n",
    "    def retrieve_context(self, query, top_k=3, min_similarity=0.2):\n",
    "        \"\"\"Retrieve relevant context for a query\"\"\"\n",
    "        results = self.vector_db.search(query, top_k=top_k)\n",
    "        \n",
    "        # Filter by minimum similarity threshold\n",
    "        relevant_docs = [\n",
    "            result for result in results \n",
    "            if result['similarity'] >= min_similarity\n",
    "        ]\n",
    "        \n",
    "        return relevant_docs\n",
    "    \n",
    "    def generate_response(self, query, context_docs):\n",
    "        \"\"\"Generate a response using retrieved context (simplified)\"\"\"\n",
    "        if not context_docs:\n",
    "            return \"I don't have relevant information to answer this question.\"\n",
    "        \n",
    "        # In production, this would call an LLM API\n",
    "        # For demo, we'll create a structured response\n",
    "        response = f\"Based on the relevant documentation, here's what I found:\\n\\n\"\n",
    "        \n",
    "        for i, doc in enumerate(context_docs, 1):\n",
    "            response += f\"{i}. {doc['document']} (relevance: {doc['similarity']:.2f})\\n\"\n",
    "        \n",
    "        response += f\"\\nThese documents should help with: {query}\"\n",
    "        return response\n",
    "    \n",
    "    def answer_question(self, question):\n",
    "        \"\"\"Complete RAG pipeline\"\"\"\n",
    "        print(f\"ü§ñ Question: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Retrieve relevant context\n",
    "        context = self.retrieve_context(question)\n",
    "        print(f\"üìö Retrieved {len(context)} relevant documents\")\n",
    "        \n",
    "        # Step 2: Generate response (simulated)\n",
    "        response = self.generate_response(question, context)\n",
    "        print(f\"\\nüí° Response:\\n{response}\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Create RAG system\n",
    "rag_system = SimpleRAG(vector_db)\n",
    "\n",
    "# Test questions\n",
    "questions = [\n",
    "    \"How do I deploy my application?\",\n",
    "    \"What's the best way to manage state in frontend?\",\n",
    "    \"How can I optimize database performance?\",\n",
    "    \"What testing strategies should I use?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    rag_system.answer_question(question)\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fedf8f4",
   "metadata": {},
   "source": [
    "## 7. Performance Comparison: Traditional vs Vector Search\n",
    "\n",
    "Let's compare traditional keyword-based search with our vector-based semantic search to see the difference in results quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407abc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(query, documents, top_k=3):\n",
    "    \"\"\"Traditional keyword-based search\"\"\"\n",
    "    query_words = set(query.lower().split())\n",
    "    \n",
    "    # Calculate keyword overlap scores\n",
    "    scores = []\n",
    "    for i, doc in enumerate(documents):\n",
    "        doc_words = set(doc.lower().split())\n",
    "        overlap = len(query_words.intersection(doc_words))\n",
    "        scores.append((i, overlap, doc))\n",
    "    \n",
    "    # Sort by overlap score\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return [{'document': score[2], 'score': score[1], 'index': score[0]} \n",
    "            for score in scores[:top_k]]\n",
    "\n",
    "def compare_search_methods(query):\n",
    "    \"\"\"Compare keyword vs vector search\"\"\"\n",
    "    print(f\"\\nüîç Query: '{query}'\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Keyword search\n",
    "    keyword_results = keyword_search(query, documents)\n",
    "    print(\"üìù Traditional Keyword Search Results:\")\n",
    "    for i, result in enumerate(keyword_results, 1):\n",
    "        print(f\"   {i}. [Score: {result['score']}] {result['document']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Vector search  \n",
    "    vector_results = vector_db.search(query, top_k=3)\n",
    "    print(\"üß† Vector Semantic Search Results:\")\n",
    "    for i, result in enumerate(vector_results, 1):\n",
    "        print(f\"   {i}. [Similarity: {result['similarity']:.3f}] {result['document']}\")\n",
    "\n",
    "# Test queries that show the difference\n",
    "test_queries = [\n",
    "    \"building web apps\",  # No exact matches but clear semantic intent\n",
    "    \"performance optimization\",  # Semantic concept\n",
    "    \"team workflow\",  # Abstract concept\n",
    "    \"frontend development\"  # Semantic category\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    compare_search_methods(query)\n",
    "    print(\"\\n\" + \"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5143283c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of search method effectiveness\n",
    "print(\"üìä Key Differences Between Search Methods:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"üî§ Keyword Search:\")\n",
    "print(\"   ‚Ä¢ Exact word matching only\")\n",
    "print(\"   ‚Ä¢ Misses synonyms and related concepts\")\n",
    "print(\"   ‚Ä¢ Simple to implement and understand\") \n",
    "print(\"   ‚Ä¢ Fast for exact matches\")\n",
    "print(\"   ‚Ä¢ Poor with typos or alternative phrasings\")\n",
    "\n",
    "print(\"\\nüß† Vector Search:\")\n",
    "print(\"   ‚Ä¢ Understands semantic meaning\")\n",
    "print(\"   ‚Ä¢ Finds related concepts even without exact words\")\n",
    "print(\"   ‚Ä¢ Better for natural language queries\")\n",
    "print(\"   ‚Ä¢ Handles synonyms and context\")\n",
    "print(\"   ‚Ä¢ More computationally intensive\")\n",
    "\n",
    "print(\"\\nüí° When to Use Each:\")\n",
    "print(\"   ‚Ä¢ Keyword: Exact term lookup, structured data\")\n",
    "print(\"   ‚Ä¢ Vector: Natural language, concept search, AI applications\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad3df74",
   "metadata": {},
   "source": [
    "## 8. Production Considerations with ChromaDB\n",
    "\n",
    "For production applications, you'll want a more robust vector database. Let's explore ChromaDB, a production-ready option that's easy to get started with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb50864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production-ready vector database with ChromaDB\n",
    "try:\n",
    "    import chromadb\n",
    "    from chromadb.config import Settings\n",
    "    \n",
    "    # Create a persistent ChromaDB client\n",
    "    client = chromadb.Client(Settings(\n",
    "        chroma_db_impl=\"duckdb+parquet\",\n",
    "        persist_directory=\"./chroma_db\"  # Persist data to disk\n",
    "    ))\n",
    "    \n",
    "    # Create or get a collection\n",
    "    collection_name = \"software_docs\"\n",
    "    collection = client.get_or_create_collection(\n",
    "        name=collection_name,\n",
    "        metadata={\"description\": \"Software engineering documentation\"}\n",
    "    )\n",
    "    \n",
    "    # Add documents with metadata\n",
    "    doc_ids = [f\"doc_{i}\" for i in range(len(documents))]\n",
    "    doc_metadata = [\n",
    "        {\"category\": \"web\", \"language\": \"python\"} if \"Django\" in doc or \"React\" in doc\n",
    "        else {\"category\": \"database\", \"language\": \"sql\"} if \"SQL\" in doc or \"database\" in doc\n",
    "        else {\"category\": \"devops\", \"language\": \"general\"} if \"Docker\" in doc or \"AWS\" in doc or \"Kubernetes\" in doc\n",
    "        else {\"category\": \"development\", \"language\": \"general\"}\n",
    "        for doc in documents\n",
    "    ]\n",
    "    \n",
    "    # Add documents to collection\n",
    "    collection.add(\n",
    "        documents=documents,\n",
    "        metadatas=doc_metadata,\n",
    "        ids=doc_ids\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Added {len(documents)} documents to ChromaDB collection\")\n",
    "    print(f\"üìä Collection stats: {collection.count()} total documents\")\n",
    "    \n",
    "    # Demonstrate advanced querying with metadata filtering\n",
    "    def advanced_search(query, category_filter=None, n_results=3):\n",
    "        \"\"\"Advanced search with metadata filtering\"\"\"\n",
    "        where_clause = {\"category\": category_filter} if category_filter else None\n",
    "        \n",
    "        results = collection.query(\n",
    "            query_texts=[query],\n",
    "            n_results=n_results,\n",
    "            where=where_clause\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nüîç Query: '{query}'\")\n",
    "        if category_filter:\n",
    "            print(f\"   üìÇ Filtered by category: {category_filter}\")\n",
    "            \n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0], \n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        ), 1):\n",
    "            similarity = 1 - distance  # Convert distance to similarity\n",
    "            print(f\"   {i}. [{similarity:.3f}] {doc}\")\n",
    "            print(f\"      üìÅ {metadata['category']} | üíª {metadata['language']}\")\n",
    "    \n",
    "    # Test advanced search capabilities\n",
    "    advanced_search(\"web development\")\n",
    "    advanced_search(\"database optimization\", category_filter=\"database\")\n",
    "    advanced_search(\"deployment\", category_filter=\"devops\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ùå ChromaDB not available. Install with: pip install chromadb\")\n",
    "    print(\"üí° Continuing with our simple vector database instead...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52006856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Production Scaling Considerations\n",
    "print(\"üöÄ Production Vector Database Considerations\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "considerations = {\n",
    "    \"Performance\": [\n",
    "        \"‚Ä¢ Index type selection (HNSW, IVF, etc.)\",\n",
    "        \"‚Ä¢ Batch operations for large datasets\", \n",
    "        \"‚Ä¢ Caching frequently accessed vectors\",\n",
    "        \"‚Ä¢ GPU acceleration for large-scale search\"\n",
    "    ],\n",
    "    \"Scalability\": [\n",
    "        \"‚Ä¢ Horizontal sharding across multiple nodes\",\n",
    "        \"‚Ä¢ Load balancing for query distribution\",\n",
    "        \"‚Ä¢ Incremental indexing for real-time updates\",\n",
    "        \"‚Ä¢ Distributed storage backends\"\n",
    "    ],\n",
    "    \"Data Management\": [\n",
    "        \"‚Ä¢ Version control for embeddings\",\n",
    "        \"‚Ä¢ Backup and disaster recovery\",\n",
    "        \"‚Ä¢ Data lifecycle management\",\n",
    "        \"‚Ä¢ Monitoring and alerting\"\n",
    "    ],\n",
    "    \"Cost Optimization\": [\n",
    "        \"‚Ä¢ Embedding model selection vs accuracy tradeoff\",\n",
    "        \"‚Ä¢ Storage optimization (compression, quantization)\",\n",
    "        \"‚Ä¢ Query optimization and caching\",\n",
    "        \"‚Ä¢ Resource usage monitoring\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in considerations.items():\n",
    "    print(f\"\\nüìã {category}:\")\n",
    "    for item in items:\n",
    "        print(f\"   {item}\")\n",
    "\n",
    "print(f\"\\nüõ†Ô∏è Popular Production Vector Databases:\")\n",
    "databases = [\n",
    "    \"‚Ä¢ ChromaDB - Open source, easy to start\",\n",
    "    \"‚Ä¢ Pinecone - Managed, serverless\",\n",
    "    \"‚Ä¢ Weaviate - GraphQL interface, multi-modal\",\n",
    "    \"‚Ä¢ Qdrant - High performance, Rust-based\",\n",
    "    \"‚Ä¢ Milvus - Open source, highly scalable\",\n",
    "    \"‚Ä¢ Redis Vector Search - If already using Redis\",\n",
    "    \"‚Ä¢ PostgreSQL pgvector - If already using PostgreSQL\"\n",
    "]\n",
    "\n",
    "for db in databases:\n",
    "    print(f\"   {db}\")\n",
    "\n",
    "print(f\"\\nüí° Selection Criteria:\")\n",
    "print(f\"   ‚Ä¢ Data volume and query patterns\")\n",
    "print(f\"   ‚Ä¢ Integration with existing infrastructure\") \n",
    "print(f\"   ‚Ä¢ Budget and operational complexity\")\n",
    "print(f\"   ‚Ä¢ Performance requirements\")\n",
    "print(f\"   ‚Ä¢ Team expertise and support needs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0994186d",
   "metadata": {},
   "source": [
    "## üéØ Key Takeaways\n",
    "\n",
    "### What We've Learned\n",
    "1. **Embeddings** convert text into numerical vectors that capture semantic meaning\n",
    "2. **Vector databases** enable similarity search based on meaning, not just keywords\n",
    "3. **Semantic search** finds relevant content even without exact word matches\n",
    "4. **RAG systems** combine retrieval with generation for intelligent responses\n",
    "5. **Production considerations** include performance, scalability, and cost optimization\n",
    "\n",
    "### Practical Applications for Software Teams\n",
    "\n",
    "**Documentation & Knowledge Management:**\n",
    "- Semantic search across technical documentation\n",
    "- Find related code examples and tutorials\n",
    "- Intelligent Q&A systems for team knowledge\n",
    "\n",
    "**Code Intelligence:**\n",
    "- Similar function/component discovery\n",
    "- Code documentation and comment search\n",
    "- Architecture pattern identification\n",
    "\n",
    "**Customer Support:**\n",
    "- Intelligent ticket routing and responses\n",
    "- Knowledge base search and retrieval\n",
    "- Automated response suggestions\n",
    "\n",
    "### Next Steps\n",
    "1. **Start Small**: Try ChromaDB or similar tools with your existing documentation\n",
    "2. **Experiment**: Test different embedding models for your domain\n",
    "3. **Measure**: Compare search quality vs traditional methods\n",
    "4. **Scale**: Consider production requirements as usage grows\n",
    "5. **Integrate**: Connect with LLMs for full RAG capabilities\n",
    "\n",
    "---\n",
    "\n",
    "### Resources for Further Learning\n",
    "- [ChromaDB Documentation](https://docs.trychroma.com/)\n",
    "- [Sentence Transformers Models](https://huggingface.co/sentence-transformers)\n",
    "- [Vector Database Comparison](https://github.com/erikbern/ann-benchmarks)\n",
    "- [RAG Best Practices](https://docs.anthropic.com/claude/docs/building-with-claude)\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Vector databases are a powerful tool for making AI applications more context-aware and intelligent. Start with simple use cases and gradually expand as you gain experience! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.11.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
